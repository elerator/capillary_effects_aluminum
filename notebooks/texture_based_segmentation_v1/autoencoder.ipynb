{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A variational autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "from tensorflow.keras.layers import Lambda, Input, Dense\n",
    "#from tensorflow.keras.models import Model\n",
    "from keras import Model\n",
    "\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.losses import mse, binary_crossentropy\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras import backend as K\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import *\n",
    "from keras import layers, models\n",
    "\n",
    "def to_functional_model(seqmodel):\n",
    "    input_layer = Input(batch_shape=seqmodel.layers[0].input_shape)\n",
    "    prev_layer = input_layer\n",
    "    for layer in seqmodel.layers:\n",
    "        prev_layer = layer(prev_layer)\n",
    "    output_layer = prev_layer\n",
    "    funcmodel = models.Model([input_layer], [prev_layer])\n",
    "    return input_layer, output_layer, funcmodel\n",
    "\n",
    "def sampling(args):\n",
    "    \"\"\"Reparameterization trick by sampling from an isotropic unit Gaussian.\n",
    "    # Arguments:\n",
    "        args (tensor): mean and log of variance of Q(z|X)\n",
    "    # Returns:\n",
    "        z (tensor): sampled latent vector\n",
    "    \"\"\"\n",
    "    z_mean, z_log_var = args\n",
    "    batch = K.shape(z_mean)[0]\n",
    "    dim = K.int_shape(z_mean)[1]\n",
    "    epsilon = K.random_normal(shape=(batch, dim))\n",
    "    return z_mean + K.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "def get_vae_loss(inputs, outputs, z_mean, z_log_var, reconstruction_loss = \"mse\", original_dim = 130, impact_reconstruction_loss = 1):\n",
    "    # VAE loss = mse_loss or binary_crossentropy + kl_loss\n",
    "    if reconstruction_loss == \"binary_crossentropy\":\n",
    "        reconstruction_loss = binary_crossentropy(inputs,outputs)\n",
    "    elif reconstruction_loss == \"mse\":\n",
    "        reconstruction_loss = mse(inputs, outputs)\n",
    "\n",
    "    reconstruction_loss *= original_dim\n",
    "    \n",
    "    kl_loss = 1 + z_log_var - K.square(z_mean) - K.exp(z_log_var)\n",
    "    kl_loss = K.sum(kl_loss, axis=-1)\n",
    "    kl_loss *= -0.5\n",
    "    vae_loss = K.mean(K.mean(impact_reconstruction_loss*reconstruction_loss) + K.mean(kl_loss))\n",
    "    return vae_loss\n",
    "\n",
    "def image_encoder():\n",
    "    image_model = Sequential()\n",
    "    image_model.add(Conv2D(32, (8, 8), subsample=(4, 4), input_shape=[50, 50, 1], activation = \"relu\"))\n",
    "    image_model.add(Conv2D(64, (4, 4), subsample=(2, 2), activation = \"relu\"))\n",
    "    image_model.add(Conv2D(64, (3, 3), subsample=(1, 1), activation = \"relu\"))\n",
    "    image_model.add(Flatten())\n",
    "    image_model.add(Dense(512, activation = \"relu\"))\n",
    "    image_model.add(Dense(512, activation = \"relu\"))\n",
    "    image_input, image_output, image_model = to_functional_model(image_model)\n",
    "    return image_input, image_output\n",
    "\n",
    "def encoder():\n",
    "    image_input, image_output = image_encoder()\n",
    "    z_mean = Dense(2, name='z_mean')(image_output)\n",
    "    z_log_var = Dense(2, name='z_log_var')(image_output)\n",
    "    z = Lambda(sampling, output_shape=(2,), name='z')([z_mean, z_log_var])\n",
    "    return image_input, [z_mean, z_log_var, z]\n",
    "\n",
    "def image_decoder(encoded):\n",
    "    shape = [-1,16,16, 1]\n",
    "    x = Dense(256, activation = \"relu\")(encoded)\n",
    "    x = Dense(256, activation = \"relu\")(encoded)\n",
    "    x = Reshape((shape[1], shape[2], shape[3]))(x)\n",
    "    x = layers.Conv2D(8, (6, 6), activation='relu', padding = \"same\")(x)\n",
    "    x = layers.UpSampling2D((2, 2))(x)\n",
    "    x = layers.Conv2D(5, (6, 6), activation='relu')(x)\n",
    "    x = layers.UpSampling2D((2, 2))(x)\n",
    "    x = layers.Conv2D(1, (5, 5), activation='relu')(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "def decoder():\n",
    "    latent_inputs = Input(shape=(2,), name='z_sampling')\n",
    "    image_decoded = image_decoder(latent_inputs)\n",
    "    return latent_inputs, image_decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/elerator/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:58: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (8, 8), input_shape=[50, 50, 1..., activation=\"relu\", strides=(4, 4))`\n",
      "/home/elerator/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:59: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (4, 4), activation=\"relu\", strides=(2, 2))`\n",
      "/home/elerator/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:60: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), activation=\"relu\", strides=(1, 1))`\n"
     ]
    }
   ],
   "source": [
    "image_input, [z_mean, z_log_var, z] = encoder()\n",
    "latent_inputs, image_decoded = decoder()\n",
    "encoder_model = Model(inputs = image_input, outputs = [z_mean, z_log_var, z], name = \"encoder\")\n",
    "decoder_model = Model(inputs = latent_inputs, outputs = image_decoded, name='decoder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([None, 50, 50, 1])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_decoded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_decoded = decoder_model(encoder_model(image_input)[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae = Model(inputs = image_input, outputs = image_decoded, name='vae_cnn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/elerator/anaconda3/lib/python3.7/site-packages/keras/engine/training_utils.py:819: UserWarning: Output decoder missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to decoder.\n",
      "  'be expecting any data to be passed to {0}.'.format(name))\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "vae.add_loss(get_vae_loss(image_input, image_decoded, z_mean, z_log_var, impact_reconstruction_loss = 1000))\n",
    "vae.compile(optimizer = \"adam\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "dataset_folder = \"texture_datasets\"\n",
    "source_patches = np.load(os.path.join(dataset_folder,\"source_patches.npy\"))\n",
    "labels = np.load(os.path.join(dataset_folder,\"labels.npy\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(source_patches, labels, test_size = 0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.expand_dims(x_train, -1)\n",
    "y_train = np.expand_dims(y_train, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(35528, 50, 50, 1)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "epochs = 400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/400\n",
      "1000/1000 [==============================] - 14s 14ms/step - loss: 26962.7907\n",
      "Epoch 2/400\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 11916.2411\n",
      "Epoch 3/400\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: 7306.8221\n",
      "Epoch 4/400\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 5609.1581\n",
      "Epoch 5/400\n",
      "1000/1000 [==============================] - 8s 8ms/step - loss: 4782.2308\n",
      "Epoch 6/400\n",
      "1000/1000 [==============================] - 8s 8ms/step - loss: 4295.3643\n",
      "Epoch 7/400\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 4027.0140\n",
      "Epoch 8/400\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 3832.7305\n",
      "Epoch 9/400\n",
      "1000/1000 [==============================] - 9s 9ms/step - loss: 3757.3629\n",
      "Epoch 10/400\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 3719.2612\n",
      "Epoch 11/400\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: 3694.2454\n",
      "Epoch 12/400\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 3676.8186\n",
      "Epoch 13/400\n",
      "1000/1000 [==============================] - 10s 10ms/step - loss: 3682.3638\n",
      "Epoch 14/400\n",
      "1000/1000 [==============================] - 9s 9ms/step - loss: 3666.9559\n",
      "Epoch 15/400\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 3654.9564\n",
      "Epoch 16/400\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 3644.7616\n",
      "Epoch 17/400\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: 3637.2885\n",
      "Epoch 18/400\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 3632.8329\n",
      "Epoch 19/400\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 3622.2142\n",
      "Epoch 20/400\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 3612.2797\n",
      "Epoch 21/400\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 3610.6943\n",
      "Epoch 22/400\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 3600.5833\n",
      "Epoch 23/400\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 3589.6422\n",
      "Epoch 24/400\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 3584.0907\n",
      "Epoch 25/400\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 3592.1503\n",
      "Epoch 26/400\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 3580.0872\n",
      "Epoch 27/400\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 3560.3862\n",
      "Epoch 28/400\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 3550.3349\n",
      "Epoch 29/400\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 3541.2626\n",
      "Epoch 30/400\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 3551.4501\n",
      "Epoch 31/400\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 3540.9345\n",
      "Epoch 32/400\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 3526.7379\n",
      "Epoch 33/400\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 3515.1372\n",
      "Epoch 34/400\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 3511.0948\n",
      "Epoch 35/400\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 3508.8894\n",
      "Epoch 36/400\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 3501.2932\n",
      "Epoch 37/400\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 3507.4700\n",
      "Epoch 38/400\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 3569.6592\n",
      "Epoch 39/400\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 3517.5056\n",
      "Epoch 40/400\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 3507.5097\n",
      "Epoch 41/400\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 3493.8989\n",
      "Epoch 42/400\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 3485.7338\n",
      "Epoch 43/400\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 3489.6551\n",
      "Epoch 44/400\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 3484.2488\n",
      "Epoch 45/400\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 3481.5734\n",
      "Epoch 46/400\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 3485.1895\n",
      "Epoch 47/400\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 3479.2279\n",
      "Epoch 48/400\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 3482.7489\n",
      "Epoch 49/400\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 3477.0881\n",
      "Epoch 50/400\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 3477.7739\n",
      "Epoch 51/400\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 3471.6793\n",
      "Epoch 52/400\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 3480.4481\n",
      "Epoch 53/400\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 3472.1512\n",
      "Epoch 54/400\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 3486.2634\n",
      "Epoch 55/400\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 3472.3891\n",
      "Epoch 56/400\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 3469.0216\n",
      "Epoch 57/400\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 3468.1900\n",
      "Epoch 58/400\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 3464.8101\n",
      "Epoch 59/400\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 3470.8003\n",
      "Epoch 60/400\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 3470.0269\n",
      "Epoch 61/400\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 3464.6458\n",
      "Epoch 62/400\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 3469.9090\n",
      "Epoch 63/400\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 3464.9122\n",
      "Epoch 64/400\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 3462.6024\n",
      "Epoch 65/400\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 3468.0932\n",
      "Epoch 66/400\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 3467.6492\n",
      "Epoch 67/400\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 3470.3160\n",
      "Epoch 68/400\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 3469.6250\n",
      "Epoch 69/400\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 3459.3248\n",
      "Epoch 70/400\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 3461.2303\n",
      "Epoch 71/400\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 3457.4718\n",
      "Epoch 72/400\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 3458.0896\n",
      "Epoch 73/400\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 3475.4376\n",
      "Epoch 74/400\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 3460.7362\n",
      "Epoch 75/400\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 3457.2328\n",
      "Epoch 76/400\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 3462.1873\n",
      "Epoch 77/400\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 3458.4888\n",
      "Epoch 78/400\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 3464.1451\n",
      "Epoch 79/400\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 3457.1628\n",
      "Epoch 80/400\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 3466.0277\n",
      "Epoch 81/400\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 3466.5934\n",
      "Epoch 82/400\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3460.3120\n",
      "Epoch 83/400\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3459.8077\n",
      "Epoch 84/400\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 3462.3755\n",
      "Epoch 85/400\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3480.9427\n",
      "Epoch 86/400\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3518.7214\n",
      "Epoch 87/400\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3526.2891\n",
      "Epoch 88/400\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 3480.4582\n",
      "Epoch 89/400\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3463.7817\n",
      "Epoch 90/400\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3453.6420\n",
      "Epoch 91/400\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3456.1883\n",
      "Epoch 92/400\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 3455.8343\n",
      "Epoch 93/400\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3454.5997\n",
      "Epoch 94/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3453.3973\n",
      "Epoch 95/400\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3450.6527\n",
      "Epoch 96/400\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3451.4121\n",
      "Epoch 97/400\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 3450.6168\n",
      "Epoch 98/400\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 3452.4144\n",
      "Epoch 99/400\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3462.4527\n",
      "Epoch 100/400\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3449.3492\n",
      "Epoch 101/400\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3450.3516\n",
      "Epoch 102/400\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 3461.1199\n",
      "Epoch 103/400\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3457.3969\n",
      "Epoch 104/400\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3456.6032\n",
      "Epoch 105/400\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 3445.9740\n",
      "Epoch 106/400\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 3443.0780\n",
      "Epoch 107/400\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 3450.0077\n",
      "Epoch 108/400\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 3445.1637\n",
      "Epoch 109/400\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3439.9564\n",
      "Epoch 110/400\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3443.0183\n",
      "Epoch 111/400\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3429.5101\n",
      "Epoch 112/400\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 3429.6362\n",
      "Epoch 113/400\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3420.9354\n",
      "Epoch 114/400\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3425.5073\n",
      "Epoch 115/400\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3426.2402\n",
      "Epoch 116/400\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3416.2154\n",
      "Epoch 117/400\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3407.8658\n",
      "Epoch 118/400\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3410.1709\n",
      "Epoch 119/400\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3425.1299\n",
      "Epoch 120/400\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3422.9172\n",
      "Epoch 121/400\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3410.7310\n",
      "Epoch 122/400\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3410.8542\n",
      "Epoch 123/400\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3408.5771\n",
      "Epoch 124/400\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3414.0854\n",
      "Epoch 125/400\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3413.4422\n",
      "Epoch 126/400\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3420.0956\n",
      "Epoch 127/400\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3418.9677\n",
      "Epoch 128/400\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3421.6329\n",
      "Epoch 129/400\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3458.4796\n",
      "Epoch 130/400\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3456.2980\n",
      "Epoch 131/400\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 3419.3704\n",
      "Epoch 132/400\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 3407.6293\n",
      "Epoch 133/400\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3403.3368\n",
      "Epoch 134/400\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3402.9259\n",
      "Epoch 135/400\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3399.6469\n",
      "Epoch 136/400\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3399.1951\n",
      "Epoch 137/400\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 3397.0653\n",
      "Epoch 138/400\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3397.2891\n",
      "Epoch 139/400\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 3395.4358\n",
      "Epoch 140/400\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3396.2511\n",
      "Epoch 141/400\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3394.6537\n",
      "Epoch 142/400\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3406.5128\n",
      "Epoch 143/400\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 3405.0106\n",
      "Epoch 144/400\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3399.5053\n",
      "Epoch 145/400\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 3397.9109\n",
      "Epoch 146/400\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3398.3960\n",
      "Epoch 147/400\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 3404.1021\n",
      "Epoch 148/400\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3394.7435\n",
      "Epoch 149/400\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3402.3160\n",
      "Epoch 150/400\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3396.6679\n",
      "Epoch 151/400\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3395.7313\n",
      "Epoch 152/400\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3397.7758\n",
      "Epoch 153/400\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3393.8982\n",
      "Epoch 154/400\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3392.8658\n",
      "Epoch 155/400\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3397.8917\n",
      "Epoch 156/400\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3393.0007\n",
      "Epoch 157/400\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3392.2597\n",
      "Epoch 158/400\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3393.5731\n",
      "Epoch 159/400\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3398.7519\n",
      "Epoch 160/400\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3394.7459\n",
      "Epoch 161/400\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3402.0709\n",
      "Epoch 162/400\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3404.4145\n",
      "Epoch 163/400\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3429.4553\n",
      "Epoch 164/400\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3464.2069\n",
      "Epoch 165/400\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3416.5255\n",
      "Epoch 166/400\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3405.7668\n",
      "Epoch 167/400\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 3397.8743\n",
      "Epoch 168/400\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 3391.0582\n",
      "Epoch 169/400\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 3391.5242\n",
      "Epoch 170/400\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3389.4223\n",
      "Epoch 171/400\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3390.2801\n",
      "Epoch 172/400\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3402.4918\n",
      "Epoch 173/400\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3399.6507\n",
      "Epoch 174/400\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3405.3636\n",
      "Epoch 175/400\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3432.4671\n",
      "Epoch 176/400\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3428.6529\n",
      "Epoch 177/400\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3409.3373\n",
      "Epoch 178/400\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3412.1739\n",
      "Epoch 179/400\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3405.2837\n",
      "Epoch 180/400\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3396.1070\n",
      "Epoch 181/400\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3389.9195\n",
      "Epoch 182/400\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3391.3968\n",
      "Epoch 183/400\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3392.5665\n",
      "Epoch 184/400\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3393.8248\n",
      "Epoch 185/400\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3402.6118\n",
      "Epoch 186/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3404.6671\n",
      "Epoch 187/400\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3394.0082\n",
      "Epoch 188/400\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3388.3476\n",
      "Epoch 189/400\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3389.3336\n",
      "Epoch 190/400\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3388.5758\n",
      "Epoch 191/400\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 3392.7208\n",
      "Epoch 192/400\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 3396.8133\n",
      "Epoch 193/400\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 3386.9728\n",
      "Epoch 194/400\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 3393.8568\n",
      "Epoch 195/400\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3395.1015\n",
      "Epoch 196/400\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3387.7855\n",
      "Epoch 197/400\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 3390.2047\n",
      "Epoch 198/400\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 3386.6813\n",
      "Epoch 199/400\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3385.6747\n",
      "Epoch 200/400\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3397.5331\n",
      "Epoch 201/400\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3413.0601\n",
      "Epoch 202/400\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3438.0173\n",
      "Epoch 203/400\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3418.5990\n",
      "Epoch 204/400\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3395.1936\n",
      "Epoch 205/400\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3396.3528\n",
      "Epoch 206/400\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 3390.6196\n",
      "Epoch 207/400\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3386.2450\n",
      "Epoch 208/400\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3384.1219\n",
      "Epoch 209/400\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3384.5755\n",
      "Epoch 210/400\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 3387.4019\n",
      "Epoch 211/400\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 3396.9668\n",
      "Epoch 212/400\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3412.7186\n",
      "Epoch 213/400\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3424.0409\n",
      "Epoch 214/400\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 3411.6740\n",
      "Epoch 215/400\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 3403.4448\n",
      "Epoch 216/400\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3391.0302\n",
      "Epoch 217/400\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 3382.0423\n",
      "Epoch 218/400\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3383.7293\n",
      "Epoch 219/400\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 3385.9476\n",
      "Epoch 220/400\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 3389.3108\n",
      "Epoch 221/400\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3386.3066\n",
      "Epoch 222/400\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3382.9748\n",
      "Epoch 223/400\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3383.9650\n",
      "Epoch 224/400\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3384.6183\n",
      "Epoch 225/400\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 3382.2461\n",
      "Epoch 226/400\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 3381.5574\n",
      "Epoch 227/400\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 3381.4595\n",
      "Epoch 228/400\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3380.8875\n",
      "Epoch 229/400\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 3385.4731\n",
      "Epoch 230/400\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3393.9464\n",
      "Epoch 231/400\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 3425.5209\n",
      "Epoch 232/400\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 3436.9386\n",
      "Epoch 233/400\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 3414.9866\n",
      "Epoch 234/400\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3388.2854\n",
      "Epoch 235/400\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 3381.8263\n",
      "Epoch 236/400\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 3380.6749\n",
      "Epoch 237/400\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 3382.1419\n",
      "Epoch 238/400\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 3383.6411\n",
      "Epoch 239/400\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3380.5379\n",
      "Epoch 240/400\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3382.3109\n",
      "Epoch 241/400\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 3384.0946\n",
      "Epoch 242/400\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 3382.6305\n",
      "Epoch 243/400\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3388.1458\n",
      "Epoch 244/400\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3386.5912\n",
      "Epoch 245/400\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3382.4842\n",
      "Epoch 246/400\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 3380.0340\n",
      "Epoch 247/400\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 3380.8029\n",
      "Epoch 248/400\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 3383.7406\n",
      "Epoch 249/400\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3383.5336\n",
      "Epoch 250/400\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3382.6418\n",
      "Epoch 251/400\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 3392.7819\n",
      "Epoch 252/400\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 3386.1109\n",
      "Epoch 253/400\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3384.2424\n",
      "Epoch 254/400\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 3384.3254\n",
      "Epoch 255/400\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3379.4653\n",
      "Epoch 256/400\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3384.0445\n",
      "Epoch 257/400\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3390.5694\n",
      "Epoch 258/400\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3384.0475\n",
      "Epoch 259/400\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3380.0592\n",
      "Epoch 260/400\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3379.7023\n",
      "Epoch 261/400\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3379.9576\n",
      "Epoch 262/400\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3378.3209\n",
      "Epoch 263/400\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3380.6573\n",
      "Epoch 264/400\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3379.1597\n",
      "Epoch 265/400\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 3378.8258\n",
      "Epoch 266/400\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: 3378.9210\n",
      "Epoch 267/400\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 3382.2049\n",
      "Epoch 268/400\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 3384.6698\n",
      "Epoch 269/400\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 3379.8256\n",
      "Epoch 270/400\n",
      "1000/1000 [==============================] - 13s 13ms/step - loss: 3379.2732\n",
      "Epoch 271/400\n",
      "1000/1000 [==============================] - 10s 10ms/step - loss: 3381.9878\n",
      "Epoch 272/400\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 3379.4639\n",
      "Epoch 273/400\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 3381.3312\n",
      "Epoch 274/400\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 3382.7350\n",
      "Epoch 275/400\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 3379.7948\n",
      "Epoch 276/400\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 3377.0071\n",
      "Epoch 277/400\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 3376.3967\n",
      "Epoch 278/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 3s 3ms/step - loss: 3376.8123\n",
      "Epoch 279/400\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 3377.1042\n",
      "Epoch 280/400\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 3380.1238\n",
      "Epoch 281/400\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 3381.2144\n",
      "Epoch 282/400\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 3381.6541\n",
      "Epoch 283/400\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 3380.3525\n",
      "Epoch 284/400\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 3378.5116\n",
      "Epoch 285/400\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 3381.6022\n",
      "Epoch 286/400\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 3379.7429\n",
      "Epoch 287/400\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 3384.0309\n",
      "Epoch 288/400\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 3387.5127\n",
      "Epoch 289/400\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 3383.0700\n",
      "Epoch 290/400\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3384.1425\n",
      "Epoch 291/400\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 3381.6637\n",
      "Epoch 292/400\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 3380.7500\n",
      "Epoch 293/400\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 3378.9199\n",
      "Epoch 294/400\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 3381.8895\n",
      "Epoch 295/400\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 3378.0258\n",
      "Epoch 296/400\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 3375.8261\n",
      "Epoch 297/400\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 3377.5862\n",
      "Epoch 298/400\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 3377.0120\n",
      "Epoch 299/400\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 3379.4392\n",
      "Epoch 300/400\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 3377.5658\n",
      "Epoch 301/400\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 3393.6947\n",
      "Epoch 302/400\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 3387.7573\n",
      "Epoch 303/400\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 3388.0702\n",
      "Epoch 304/400\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 3384.6874\n",
      "Epoch 305/400\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 3386.2698\n",
      "Epoch 306/400\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 3381.3769\n",
      "Epoch 307/400\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 3384.6069\n",
      "Epoch 308/400\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 3378.4749\n",
      "Epoch 309/400\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 3375.2768\n",
      "Epoch 310/400\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 3374.4383\n",
      "Epoch 311/400\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 3375.0453\n",
      "Epoch 312/400\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 3382.0183\n",
      "Epoch 313/400\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3377.5470\n",
      "Epoch 314/400\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 3373.4475\n",
      "Epoch 315/400\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 3372.4697\n",
      "Epoch 316/400\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 3374.8733\n",
      "Epoch 317/400\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 3376.5920\n",
      "Epoch 318/400\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 3374.0553\n",
      "Epoch 319/400\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 3372.9296\n",
      "Epoch 320/400\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3373.3016\n",
      "Epoch 321/400\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3383.8771\n",
      "Epoch 322/400\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 3381.7810\n",
      "Epoch 323/400\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 3386.8531\n",
      "Epoch 324/400\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 3379.2272\n",
      "Epoch 325/400\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3379.4453\n",
      "Epoch 326/400\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 3380.8859\n",
      "Epoch 327/400\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 3381.2344\n",
      "Epoch 328/400\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3385.4321\n",
      "Epoch 329/400\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3382.1727\n",
      "Epoch 330/400\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3380.6226\n",
      "Epoch 331/400\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3378.8417\n",
      "Epoch 332/400\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3372.1752\n",
      "Epoch 333/400\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3371.5891\n",
      "Epoch 334/400\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3371.3801\n",
      "Epoch 335/400\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3372.9094\n",
      "Epoch 336/400\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3372.8958\n",
      "Epoch 337/400\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3372.4641\n",
      "Epoch 338/400\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3376.8240\n",
      "Epoch 339/400\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3382.0980\n",
      "Epoch 340/400\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3376.9552\n",
      "Epoch 341/400\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3371.5492\n",
      "Epoch 342/400\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3378.0742\n",
      "Epoch 343/400\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3386.9766\n",
      "Epoch 344/400\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3390.6903\n",
      "Epoch 345/400\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3393.8723\n",
      "Epoch 346/400\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3381.6183\n",
      "Epoch 347/400\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 3379.0037\n",
      "Epoch 348/400\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 3373.3714\n",
      "Epoch 349/400\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 3370.4630\n",
      "Epoch 350/400\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3370.4162\n",
      "Epoch 351/400\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3370.0568\n",
      "Epoch 352/400\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 3372.4798\n",
      "Epoch 353/400\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3370.0413\n",
      "Epoch 354/400\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3370.5813\n",
      "Epoch 355/400\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 3376.4700\n",
      "Epoch 356/400\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3373.0636\n",
      "Epoch 357/400\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3368.8345\n",
      "Epoch 358/400\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3369.8031\n",
      "Epoch 359/400\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3370.2977\n",
      "Epoch 360/400\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3376.9194\n",
      "Epoch 361/400\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3378.3112\n",
      "Epoch 362/400\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3368.8690\n",
      "Epoch 363/400\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3371.9775\n",
      "Epoch 364/400\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3373.1565\n",
      "Epoch 365/400\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3375.2657\n",
      "Epoch 366/400\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3369.2118\n",
      "Epoch 367/400\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3367.0992\n",
      "Epoch 368/400\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3370.3683\n",
      "Epoch 369/400\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3372.4851\n",
      "Epoch 370/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3367.5216\n",
      "Epoch 371/400\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 3372.2045\n",
      "Epoch 372/400\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3370.5406\n",
      "Epoch 373/400\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3378.6459\n",
      "Epoch 374/400\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 3368.4869\n",
      "Epoch 375/400\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3370.3328\n",
      "Epoch 376/400\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3368.3602\n",
      "Epoch 377/400\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3373.1207\n",
      "Epoch 378/400\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3377.6709\n",
      "Epoch 379/400\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3395.7352\n",
      "Epoch 380/400\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3392.1190\n",
      "Epoch 381/400\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3405.1189\n",
      "Epoch 382/400\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3469.3442\n",
      "Epoch 383/400\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3420.9758\n",
      "Epoch 384/400\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3396.0663\n",
      "Epoch 385/400\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3390.1987\n",
      "Epoch 386/400\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3381.3707\n",
      "Epoch 387/400\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3373.4341\n",
      "Epoch 388/400\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3371.8765\n",
      "Epoch 389/400\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3375.5899\n",
      "Epoch 390/400\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3366.0255\n",
      "Epoch 391/400\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3363.5562\n",
      "Epoch 392/400\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3365.3454\n",
      "Epoch 393/400\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3365.2988\n",
      "Epoch 394/400\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3363.3503\n",
      "Epoch 395/400\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3363.5227\n",
      "Epoch 396/400\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3362.3625\n",
      "Epoch 397/400\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3361.8164\n",
      "Epoch 398/400\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3362.3202\n",
      "Epoch 399/400\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3361.3599\n",
      "Epoch 400/400\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3361.4977\n"
     ]
    }
   ],
   "source": [
    "history = vae.fit(x_train[:1000], epochs=epochs, batch_size=batch_size, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-6f3a92b3a110>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvae\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"loss\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "plt.plot(vae.history.history[\"loss\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = vae.fit(x_train[1000:2000], epochs=epochs, batch_size=batch_size, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = vae.fit(x_train, epochs=1, batch_size=batch_size, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(vae.history.history[\"loss\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the autoencoder\n",
    "#history = vae.fit(x_train, epochs=epochs, batch_size=batch_size, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = encoder_model.predict(x_train)[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import gaussian_kde"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1)\n",
    "xy = np.vstack([res.T[0], res.T[1]])\n",
    "z = gaussian_kde(xy)(xy)\n",
    "\n",
    "print(len(z))\n",
    "\n",
    "ax.scatter(res.T[0], res.T[1], c = z, edgecolor = \"\")\n",
    "xrange = [np.min(res.T[0]), np.max(res.T[0])]\n",
    "yrange = [np.min(res.T[1]), np.max(res.T[1])]\n",
    "ax.set_xlim(xrange)\n",
    "ax.set_ylim(yrange)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1)\n",
    "\n",
    "ax.scatter(res.T[0][~y_train[:,0]], res.T[1][~y_train[:,0]], c = \"yellow\", edgecolor = \"\", s = 5, label = \"full\")\n",
    "ax.scatter(res.T[0][y_train[:,0]], res.T[1][y_train[:,0]], c = \"blue\", edgecolor = \"\", s = 1, label = \"empty\")\n",
    "ax.legend()\n",
    "\n",
    "xrange = [np.min(res.T[0]), np.max(res.T[0])]\n",
    "yrange = [np.min(res.T[1]), np.max(res.T[1])]\n",
    "ax.set_xlim(xrange)\n",
    "ax.set_ylim(yrange)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 25\n",
    "xs = np.linspace(xrange[0],xrange[1],n)\n",
    "ys = np.linspace(yrange[0],yrange[1],n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = np.array([[0,0,0],[0,0,0]]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(n,n, figsize= (n,n))\n",
    "for i in range(len(list(ax))):\n",
    "    for i1 in range(len(list(ax)[0])):\n",
    "        ax[i, i1].axis(\"off\")\n",
    "        ax[i, i1].imshow(decoder_model.predict(np.array([[xs[i]], \n",
    "                                                         [ys[i]]]).T)[0,:,:,0], vmin = .25, vmax = .75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
