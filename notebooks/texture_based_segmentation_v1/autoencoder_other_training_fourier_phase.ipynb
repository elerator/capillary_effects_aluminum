{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A variational autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "from tensorflow.keras.layers import Lambda, Input, Dense\n",
    "#from tensorflow.keras.models import Model\n",
    "from keras import Model\n",
    "\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.losses import mse, binary_crossentropy\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras import backend as K\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import *\n",
    "from keras import layers, models\n",
    "\n",
    "def to_functional_model(seqmodel):\n",
    "    input_layer = Input(batch_shape=seqmodel.layers[0].input_shape)\n",
    "    prev_layer = input_layer\n",
    "    for layer in seqmodel.layers:\n",
    "        prev_layer = layer(prev_layer)\n",
    "    output_layer = prev_layer\n",
    "    funcmodel = models.Model([input_layer], [prev_layer])\n",
    "    return input_layer, output_layer, funcmodel\n",
    "\n",
    "def sampling(args):\n",
    "    \"\"\"Reparameterization trick by sampling from an isotropic unit Gaussian.\n",
    "    # Arguments:\n",
    "        args (tensor): mean and log of variance of Q(z|X)\n",
    "    # Returns:\n",
    "        z (tensor): sampled latent vector\n",
    "    \"\"\"\n",
    "    z_mean, z_log_var = args\n",
    "    batch = K.shape(z_mean)[0]\n",
    "    dim = K.int_shape(z_mean)[1]\n",
    "    epsilon = K.random_normal(shape=(batch, dim))\n",
    "    return z_mean + K.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "def get_vae_loss(inputs, outputs, z_mean, z_log_var, reconstruction_loss = \"mse\", original_dim = 130, impact_reconstruction_loss = 1, impact_kl_loss = 1):\n",
    "    # VAE loss = mse_loss or binary_crossentropy + kl_loss\n",
    "    if reconstruction_loss == \"binary_crossentropy\":\n",
    "        reconstruction_loss = binary_crossentropy(inputs,outputs)\n",
    "    elif reconstruction_loss == \"mse\":\n",
    "        reconstruction_loss = mse(inputs, outputs)\n",
    "\n",
    "    reconstruction_loss *= original_dim\n",
    "    \n",
    "    kl_loss = 1 + z_log_var - K.square(z_mean) - K.exp(z_log_var)\n",
    "    kl_loss = K.sum(kl_loss, axis=-1)\n",
    "    kl_loss *= -0.5\n",
    "    vae_loss = K.mean(K.mean(impact_reconstruction_loss*reconstruction_loss) + impact_kl_loss * K.mean(kl_loss))\n",
    "    return vae_loss\n",
    "\n",
    "def image_encoder():\n",
    "    image_model = Sequential()\n",
    "    image_model.add(Conv2D(32, (8, 8), subsample=(4, 4), input_shape=[50, 50, 1], activation = \"relu\"))\n",
    "    image_model.add(Conv2D(64, (4, 4), subsample=(2, 2), activation = \"relu\"))\n",
    "    image_model.add(Conv2D(64, (3, 3), subsample=(1, 1), activation = \"relu\"))\n",
    "    image_model.add(Flatten())\n",
    "    image_model.add(Dense(512, activation = \"relu\"))\n",
    "    image_model.add(Dense(256, activation = \"relu\"))\n",
    "    image_model.add(Dense(64, activation = \"relu\"))\n",
    "\n",
    "    image_input, image_output, image_model = to_functional_model(image_model)\n",
    "    return image_input, image_output\n",
    "\n",
    "def encoder():\n",
    "    image_input, image_output = image_encoder()\n",
    "    z_mean = Dense(2, name='z_mean')(image_output)\n",
    "    z_log_var = Dense(2, name='z_log_var')(image_output)\n",
    "    z = Lambda(sampling, output_shape=(2,), name='z')([z_mean, z_log_var])\n",
    "    return image_input, [z_mean, z_log_var, z]\n",
    "\n",
    "def image_decoder(encoded):\n",
    "    shape = [-1,16,16, 1]\n",
    "    x = Dense(256, activation = \"relu\")(encoded)\n",
    "    x = Dense(256, activation = \"relu\")(encoded)\n",
    "    x = Reshape((shape[1], shape[2], shape[3]))(x)\n",
    "    x = layers.Conv2D(8, (6, 6), activation='relu', padding = \"same\")(x)\n",
    "    x = layers.UpSampling2D((2, 2))(x)\n",
    "    x = layers.Conv2D(5, (6, 6), activation='relu')(x)\n",
    "    x = layers.UpSampling2D((2, 2))(x)\n",
    "    x = layers.Conv2D(1, (5, 5), activation='relu')(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "def decoder():\n",
    "    latent_inputs = Input(shape=(2,), name='z_sampling')\n",
    "    image_decoded = image_decoder(latent_inputs)\n",
    "    return latent_inputs, image_decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/elerator/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:58: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (8, 8), input_shape=[50, 50, 1..., activation=\"relu\", strides=(4, 4))`\n",
      "/home/elerator/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:59: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (4, 4), activation=\"relu\", strides=(2, 2))`\n",
      "/home/elerator/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:60: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), activation=\"relu\", strides=(1, 1))`\n"
     ]
    }
   ],
   "source": [
    "image_input, [z_mean, z_log_var, z] = encoder()\n",
    "latent_inputs, image_decoded = decoder()\n",
    "encoder_model = Model(inputs = image_input, outputs = [z_mean, z_log_var, z], name = \"encoder\")\n",
    "decoder_model = Model(inputs = latent_inputs, outputs = image_decoded, name='decoder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([None, 50, 50, 1])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_decoded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_decoded = decoder_model(encoder_model(image_input)[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae = Model(inputs = image_input, outputs = image_decoded, name='vae_cnn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/elerator/anaconda3/lib/python3.7/site-packages/keras/engine/training_utils.py:819: UserWarning: Output decoder missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to decoder.\n",
      "  'be expecting any data to be passed to {0}.'.format(name))\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "vae.add_loss(get_vae_loss(image_input, image_decoded, z_mean, z_log_var, impact_reconstruction_loss = 1, impact_kl_loss = 0))\n",
    "vae.compile(optimizer = \"adam\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "dataset_folder = \"texture_datasets\"\n",
    "source_patches = np.load(os.path.join(dataset_folder,\"source_patches.npy\"))\n",
    "labels = np.load(os.path.join(dataset_folder,\"labels.npy\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(source_patches, labels, test_size = 0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.expand_dims(x_train, -1)\n",
    "y_train = np.expand_dims(y_train, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(35528, 50, 50, 1)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "1000/1000 [==============================] - 9s 9ms/step - loss: 30.0952\n",
      "Epoch 2/200\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 16.7363\n",
      "Epoch 3/200\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 8.6576\n",
      "Epoch 4/200\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 6.1902\n",
      "Epoch 5/200\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 5.2286\n",
      "Epoch 6/200\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 4.6857\n",
      "Epoch 7/200\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 4.2444\n",
      "Epoch 8/200\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 4.0452\n",
      "Epoch 9/200\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 3.8978\n",
      "Epoch 10/200\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 3.8262\n",
      "Epoch 11/200\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 3.7648\n",
      "Epoch 12/200\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 3.7236\n",
      "Epoch 13/200\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 3.6993\n",
      "Epoch 14/200\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 3.6751\n",
      "Epoch 15/200\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 3.6638\n",
      "Epoch 16/200\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 3.6500\n",
      "Epoch 17/200\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 3.6390\n",
      "Epoch 18/200\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 3.6324\n",
      "Epoch 19/200\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 3.6257\n",
      "Epoch 20/200\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3.6245\n",
      "Epoch 21/200\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3.6164\n",
      "Epoch 22/200\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3.6104\n",
      "Epoch 23/200\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3.6054\n",
      "Epoch 24/200\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3.6040\n",
      "Epoch 25/200\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3.6013\n",
      "Epoch 26/200\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 3.5940\n",
      "Epoch 27/200\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 3.5903\n",
      "Epoch 28/200\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 3.5862\n",
      "Epoch 29/200\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 3.5814\n",
      "Epoch 30/200\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 3.5774\n",
      "Epoch 31/200\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 3.5756\n",
      "Epoch 32/200\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 3.5696\n",
      "Epoch 33/200\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3.5663\n",
      "Epoch 34/200\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3.5634\n",
      "Epoch 35/200\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3.5669\n",
      "Epoch 36/200\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 3.5569\n",
      "Epoch 37/200\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 3.5514\n",
      "Epoch 38/200\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 3.5513\n",
      "Epoch 39/200\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3.5499\n",
      "Epoch 40/200\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3.5421\n",
      "Epoch 41/200\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3.5382\n",
      "Epoch 42/200\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 3.5365\n",
      "Epoch 43/200\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 3.5313\n",
      "Epoch 44/200\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 3.5301\n",
      "Epoch 45/200\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3.5254\n",
      "Epoch 46/200\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3.5208\n",
      "Epoch 47/200\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3.5210\n",
      "Epoch 48/200\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 3.5212\n",
      "Epoch 49/200\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 3.5159\n",
      "Epoch 50/200\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 3.5086\n",
      "Epoch 51/200\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 3.5078\n",
      "Epoch 52/200\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 3.5099\n",
      "Epoch 53/200\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 3.5033\n",
      "Epoch 54/200\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3.5061\n",
      "Epoch 55/200\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3.4987\n",
      "Epoch 56/200\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 3.5012\n",
      "Epoch 57/200\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 3.4920\n",
      "Epoch 58/200\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 3.4908\n",
      "Epoch 59/200\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3.4903\n",
      "Epoch 60/200\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3.4870\n",
      "Epoch 61/200\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 3.4842\n",
      "Epoch 62/200\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 3.4837\n",
      "Epoch 63/200\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3.4869\n",
      "Epoch 64/200\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3.4858\n",
      "Epoch 65/200\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3.4815\n",
      "Epoch 66/200\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 3.4770\n",
      "Epoch 67/200\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 3.4750\n",
      "Epoch 68/200\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3.4735\n",
      "Epoch 69/200\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3.4716\n",
      "Epoch 70/200\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3.4725\n",
      "Epoch 71/200\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3.4722\n",
      "Epoch 72/200\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3.4689\n",
      "Epoch 73/200\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 3.4680\n",
      "Epoch 74/200\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 3.4702\n",
      "Epoch 75/200\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 3.4652\n",
      "Epoch 76/200\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 3.4746\n",
      "Epoch 77/200\n",
      "1000/1000 [==============================] - 8s 8ms/step - loss: 3.4755\n",
      "Epoch 78/200\n",
      "1000/1000 [==============================] - 14s 14ms/step - loss: 3.4722\n",
      "Epoch 79/200\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 3.4675\n",
      "Epoch 80/200\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 3.4727\n",
      "Epoch 81/200\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 3.4659\n",
      "Epoch 82/200\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 3.4628\n",
      "Epoch 83/200\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 3.4610\n",
      "Epoch 84/200\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 3.4622\n",
      "Epoch 85/200\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 3.4642\n",
      "Epoch 86/200\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 3.4643\n",
      "Epoch 87/200\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 3.4696\n",
      "Epoch 88/200\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3.4775\n",
      "Epoch 89/200\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 3.4934\n",
      "Epoch 90/200\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 3.4911\n",
      "Epoch 91/200\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3.4758\n",
      "Epoch 92/200\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3.4714\n",
      "Epoch 93/200\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 3.4720\n",
      "Epoch 94/200\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 3.4600\n",
      "Epoch 95/200\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 3.4563\n",
      "Epoch 96/200\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3.4599\n",
      "Epoch 97/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3.4556\n",
      "Epoch 98/200\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 3.4563\n",
      "Epoch 99/200\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 3.4569\n",
      "Epoch 100/200\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 3.4568\n",
      "Epoch 101/200\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3.4609\n",
      "Epoch 102/200\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3.4638\n",
      "Epoch 103/200\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3.4590\n",
      "Epoch 104/200\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 3.4602\n",
      "Epoch 105/200\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 3.4551\n",
      "Epoch 106/200\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 3.4544\n",
      "Epoch 107/200\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3.4517\n",
      "Epoch 108/200\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3.4526\n",
      "Epoch 109/200\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 3.4586\n",
      "Epoch 110/200\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 3.4604\n",
      "Epoch 111/200\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 3.4549\n",
      "Epoch 112/200\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3.4518\n",
      "Epoch 113/200\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3.4554\n",
      "Epoch 114/200\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3.4705\n",
      "Epoch 115/200\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 3.4752\n",
      "Epoch 116/200\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 3.4715\n",
      "Epoch 117/200\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 3.4631\n",
      "Epoch 118/200\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3.4583\n",
      "Epoch 119/200\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3.4525\n",
      "Epoch 120/200\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3.4490\n",
      "Epoch 121/200\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3.4494\n",
      "Epoch 122/200\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3.4481\n",
      "Epoch 123/200\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3.4492\n",
      "Epoch 124/200\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 3.4496\n",
      "Epoch 125/200\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 3.4562\n",
      "Epoch 126/200\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3.4690\n",
      "Epoch 127/200\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3.5032\n",
      "Epoch 128/200\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3.4760\n",
      "Epoch 129/200\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 3.4538\n",
      "Epoch 130/200\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 3.4506\n",
      "Epoch 131/200\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 3.4483\n",
      "Epoch 132/200\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3.4468\n",
      "Epoch 133/200\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3.4457\n",
      "Epoch 134/200\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3.4473\n",
      "Epoch 135/200\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 3.4454\n",
      "Epoch 136/200\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 3.4462\n",
      "Epoch 137/200\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 3.4474\n",
      "Epoch 138/200\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3.4550\n",
      "Epoch 139/200\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 3.4530\n",
      "Epoch 140/200\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 3.4462\n",
      "Epoch 141/200\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 3.4437\n",
      "Epoch 142/200\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3.4456\n",
      "Epoch 143/200\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3.4473\n",
      "Epoch 144/200\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 3.4449\n",
      "Epoch 145/200\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 3.4467\n",
      "Epoch 146/200\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 3.4428\n",
      "Epoch 147/200\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3.4437\n",
      "Epoch 148/200\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3.4407\n",
      "Epoch 149/200\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3.4472\n",
      "Epoch 150/200\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3.4453\n",
      "Epoch 151/200\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 3.4480\n",
      "Epoch 152/200\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 3.4373\n",
      "Epoch 153/200\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3.4373\n",
      "Epoch 154/200\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3.4390\n",
      "Epoch 155/200\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3.4415\n",
      "Epoch 156/200\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3.4369\n",
      "Epoch 157/200\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 3.4581\n",
      "Epoch 158/200\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 3.4271\n",
      "Epoch 159/200\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 3.4215\n",
      "Epoch 160/200\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 3.4208\n",
      "Epoch 161/200\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 3.4168\n",
      "Epoch 162/200\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 3.4191\n",
      "Epoch 163/200\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 3.4380\n",
      "Epoch 164/200\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 3.4203\n",
      "Epoch 165/200\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3.4150\n",
      "Epoch 166/200\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 3.4168\n",
      "Epoch 167/200\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 3.4130\n",
      "Epoch 168/200\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3.4073\n",
      "Epoch 169/200\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 3.4026\n",
      "Epoch 170/200\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 3.4022\n",
      "Epoch 171/200\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 3.4021\n",
      "Epoch 172/200\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3.4047\n",
      "Epoch 173/200\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3.4138\n",
      "Epoch 174/200\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3.4050\n",
      "Epoch 175/200\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 3.3994\n",
      "Epoch 176/200\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 3.4127\n",
      "Epoch 177/200\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 3.4262\n",
      "Epoch 178/200\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3.4074\n",
      "Epoch 179/200\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3.4017\n",
      "Epoch 180/200\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3.4164\n",
      "Epoch 181/200\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 3.4027\n",
      "Epoch 182/200\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 3.3943\n",
      "Epoch 183/200\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3.4148\n",
      "Epoch 184/200\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3.4030\n",
      "Epoch 185/200\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3.3923\n",
      "Epoch 186/200\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3.3971\n",
      "Epoch 187/200\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 3.3974\n",
      "Epoch 188/200\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 3.3894\n",
      "Epoch 189/200\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3.3899\n",
      "Epoch 190/200\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3.3901\n",
      "Epoch 191/200\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3.3930\n",
      "Epoch 192/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3.3959\n",
      "Epoch 193/200\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 3.3925\n",
      "Epoch 194/200\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 3.3865\n",
      "Epoch 195/200\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 3.3909\n",
      "Epoch 196/200\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3.3949\n",
      "Epoch 197/200\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3.3884\n",
      "Epoch 198/200\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 3.3872\n",
      "Epoch 199/200\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 3.4016\n",
      "Epoch 200/200\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 3.3985\n"
     ]
    }
   ],
   "source": [
    "history = vae.fit(x_train[:1000], epochs = 200, batch_size=batch_size, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f50bfbb0310>]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(vae.history.history[\"loss\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 3.3698\n",
      "Epoch 2/100\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 3.3806\n",
      "Epoch 3/100\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 3.3809\n",
      "Epoch 4/100\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 3.3565\n",
      "Epoch 5/100\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3.3595\n",
      "Epoch 6/100\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 3.3576\n",
      "Epoch 7/100\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 3.3474\n",
      "Epoch 8/100\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 3.3450\n",
      "Epoch 9/100\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 3.3501\n",
      "Epoch 10/100\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 3.3449\n",
      "Epoch 11/100\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 3.3430\n",
      "Epoch 12/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 3.3441\n",
      "Epoch 13/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 3.3451\n",
      "Epoch 14/100\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 3.3506\n",
      "Epoch 15/100\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 3.3483\n",
      "Epoch 16/100\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3.3436\n",
      "Epoch 17/100\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 3.3497\n",
      "Epoch 18/100\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 3.3465\n",
      "Epoch 19/100\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 3.3456\n",
      "Epoch 20/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 3.3533\n",
      "Epoch 21/100\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: 3.3431\n",
      "Epoch 22/100\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: 3.3396\n",
      "Epoch 23/100\n",
      "1000/1000 [==============================] - 8s 8ms/step - loss: 3.3429\n",
      "Epoch 24/100\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 3.3412\n",
      "Epoch 25/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 3.3388\n",
      "Epoch 26/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 3.3388\n",
      "Epoch 27/100\n",
      "1000/1000 [==============================] - 8s 8ms/step - loss: 3.3419\n",
      "Epoch 28/100\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 3.3398\n",
      "Epoch 29/100\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: 3.3399\n",
      "Epoch 30/100\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 3.3495\n",
      "Epoch 31/100\n",
      "1000/1000 [==============================] - 10s 10ms/step - loss: 3.3503\n",
      "Epoch 32/100\n",
      "1000/1000 [==============================] - 9s 9ms/step - loss: 3.3491\n",
      "Epoch 33/100\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 3.3400\n",
      "Epoch 34/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 3.3413\n",
      "Epoch 35/100\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: 3.3447\n",
      "Epoch 36/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 3.3368\n",
      "Epoch 37/100\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 3.3382\n",
      "Epoch 38/100\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 3.3371\n",
      "Epoch 39/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 3.3362\n",
      "Epoch 40/100\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 3.3353\n",
      "Epoch 41/100\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 3.3360\n",
      "Epoch 42/100\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 3.3378\n",
      "Epoch 43/100\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: 3.3409\n",
      "Epoch 44/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 3.3469\n",
      "Epoch 45/100\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 3.3332\n",
      "Epoch 46/100\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 3.3355\n",
      "Epoch 47/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 3.3337\n",
      "Epoch 48/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 3.3411\n",
      "Epoch 49/100\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 3.3409\n",
      "Epoch 50/100\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 3.3367\n",
      "Epoch 51/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 3.3404\n",
      "Epoch 52/100\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 3.3425\n",
      "Epoch 53/100\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 3.3496\n",
      "Epoch 54/100\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 3.3399\n",
      "Epoch 55/100\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 3.3324\n",
      "Epoch 56/100\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 3.3308\n",
      "Epoch 57/100\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 3.3302\n",
      "Epoch 58/100\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 3.3296\n",
      "Epoch 59/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 3.3378\n",
      "Epoch 60/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 3.3304\n",
      "Epoch 61/100\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 3.3313\n",
      "Epoch 62/100\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 3.3316\n",
      "Epoch 63/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 3.3332\n",
      "Epoch 64/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 3.3398\n",
      "Epoch 65/100\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 3.3368\n",
      "Epoch 66/100\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 3.3328\n",
      "Epoch 67/100\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 3.3402\n",
      "Epoch 68/100\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 3.3332\n",
      "Epoch 69/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 3.3470\n",
      "Epoch 70/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 3.3325\n",
      "Epoch 71/100\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 3.3328\n",
      "Epoch 72/100\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 3.3273\n",
      "Epoch 73/100\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 3.3324\n",
      "Epoch 74/100\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 3.3302\n",
      "Epoch 75/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 3.3397\n",
      "Epoch 76/100\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 3.3378\n",
      "Epoch 77/100\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 3.3356\n",
      "Epoch 78/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 3.3264\n",
      "Epoch 79/100\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 3.3347\n",
      "Epoch 80/100\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 3.3341\n",
      "Epoch 81/100\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 3.3509\n",
      "Epoch 82/100\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 3.3473\n",
      "Epoch 83/100\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 3.3488\n",
      "Epoch 84/100\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 3.3343\n",
      "Epoch 85/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 3.3303\n",
      "Epoch 86/100\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 3.3414\n",
      "Epoch 87/100\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 3.3272\n",
      "Epoch 88/100\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 3.3262\n",
      "Epoch 89/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 3.3296\n",
      "Epoch 90/100\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 3.3348\n",
      "Epoch 91/100\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 3.3243\n",
      "Epoch 92/100\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 3.3241\n",
      "Epoch 93/100\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 3.3258\n",
      "Epoch 94/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 3.3282\n",
      "Epoch 95/100\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 3.3813\n",
      "Epoch 96/100\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 3.3477\n",
      "Epoch 97/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 4s 4ms/step - loss: 3.3304\n",
      "Epoch 98/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 3.3255\n",
      "Epoch 99/100\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 3.3341\n",
      "Epoch 100/100\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 3.3305\n"
     ]
    }
   ],
   "source": [
    "history = vae.fit(x_train[1000:2000], epochs=100, batch_size=batch_size, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = vae.fit(x_train, epochs = 70, batch_size=batch_size, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(vae.history.history[\"loss\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the autoencoder\n",
    "#history = vae.fit(x_train, epochs=epochs, batch_size=batch_size, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import gaussian_kde"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,2)\n",
    "\n",
    "res_pos_ex = encoder_model.predict(x_train[y_train[:,0]])[2]\n",
    "xy = np.vstack([res_pos_ex.T[0], res_pos_ex.T[1]])\n",
    "z = gaussian_kde(xy)(xy)\n",
    "ax[0].scatter(res_pos_ex.T[0], res_pos_ex.T[1], c = z, edgecolor = \"\")\n",
    "xrange = [np.min(res_pos_ex.T[0]), np.max(res_pos_ex.T[0])]\n",
    "yrange = [np.min(res_pos_ex.T[1]), np.max(res_pos_ex.T[1])]\n",
    "\n",
    "\n",
    "res_neg_ex = encoder_model.predict(x_train[~y_train[:,0]])[2]\n",
    "xy = np.vstack([res_neg_ex.T[0], res_neg_ex.T[1]])\n",
    "z = gaussian_kde(xy)(xy)\n",
    "ax[1].scatter(res_neg_ex.T[0], res_neg_ex.T[1], c = z, edgecolor = \"\")\n",
    "xrange = [np.min(res_neg_ex.T[0]), np.max(res_neg_ex.T[0])]\n",
    "yrange = [np.min(res_neg_ex.T[1]), np.max(res_neg_ex.T[1])]\n",
    "\n",
    "\n",
    "ax[0].set_xlim(xrange)\n",
    "ax[0].set_ylim(yrange)\n",
    "ax[0].set_aspect(1)\n",
    "\n",
    "ax[1].set_xlim(xrange)\n",
    "ax[1].set_ylim(yrange)\n",
    "ax[1].set_aspect(1)\n",
    "\n",
    "res = encoder_model.predict(x_train)[2]\n",
    "xrange = [np.percentile(res.T[0], 1), np.percentile(res.T[0], 99)]\n",
    "yrange = [np.percentile(res.T[1], 1), np.percentile(res.T[1], 99)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbins = 25\n",
    "bins = [np.linspace(xrange[0],xrange[1], nbins), np.linspace(yrange[0], yrange[1], nbins)]\n",
    "pos, neg = [np.histogram2d(res_neg_ex.T[0], res_neg_ex.T[1], bins = bins, density = False), \n",
    "                            np.histogram2d(res_pos_ex.T[0], res_pos_ex.T[1], bins = bins, density = False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 3, figsize = (10,10))\n",
    "ax[1].imshow(np.flipud(pos[0]), cmap = \"viridis\")\n",
    "ax[0].imshow(np.flipud(neg[0]), cmap = \"viridis\")\n",
    "ax[2].imshow(np.flipud(pos[0]-neg[0]), cmap = \"seismic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#res = encoder_model.predict(x_train)[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1)\n",
    "\n",
    "ax.scatter(res.T[0][~y_train[:,0]], res.T[1][~y_train[:,0]], c = \"yellow\", edgecolor = \"\", s = 5, label = \"full\")\n",
    "ax.scatter(res.T[0][y_train[:,0]], res.T[1][y_train[:,0]], c = \"blue\", edgecolor = \"\", s = 1, label = \"empty\")\n",
    "ax.legend()\n",
    "\n",
    "xrange = [np.percentile(res.T[0], 1), np.percentile(res.T[0], 99)]\n",
    "yrange = [np.percentile(res.T[1], 1), np.percentile(res.T[1], 99)]\n",
    "ax.set_xlim(xrange)\n",
    "ax.set_ylim(yrange)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fig2rgb_array(fig):\n",
    "    \"\"\" Converts a matplotlib figure to an rgb array such that it may be displayed as an ImageDisplay\n",
    "    Args:\n",
    "        fig: Matplotlib figure\n",
    "    Returns:\n",
    "        arr: Image of the plot in the form of a numpy array\n",
    "    \"\"\"\n",
    "    fig.canvas.draw()\n",
    "    buf = fig.canvas.tostring_rgb()\n",
    "    ncols, nrows = fig.canvas.get_width_height()\n",
    "    return np.fromstring(buf, dtype=np.uint8).reshape(nrows, ncols, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 25\n",
    "xs = np.linspace(xrange[0],xrange[1],n)\n",
    "ys = np.linspace(yrange[0],yrange[1],n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = np.array([[0,0,0],[0,0,0]]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(n,n, figsize= (n,n))\n",
    "plt.subplots_adjust(0,0,1,1,.05,.05)\n",
    "for i in range(len(list(ax))):\n",
    "    for i1 in range(len(list(ax)[0])):\n",
    "        ax[i, i1].axis(\"off\")\n",
    "        curr_y, curr_x = xs[i1], ys[len(ys)-1-i]\n",
    "        pred = decoder_model.predict(np.array([[curr_y,  curr_x]]))[0,:,:,0]\n",
    "        ax[i, i1].imshow(pred, vmin = .25, vmax = .75)\n",
    "manifold = fig2rgb_array(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(constrained_layout=True, figsize = (10,8), dpi = 300)\n",
    "gs = fig.add_gridspec(nrows=4, ncols=6, left=0.0, right=1, wspace = 0, hspace = 0)\n",
    "ax = []\n",
    "ax.append(fig.add_subplot(gs[:3, :3]))\n",
    "ax.append(fig.add_subplot(gs[:3, 3:]))\n",
    "ax.append(fig.add_subplot(gs[3, 0:2]))\n",
    "ax.append(fig.add_subplot(gs[3, 2:4]))\n",
    "ax.append(fig.add_subplot(gs[3, 4:6]))\n",
    "#fig, ax = plt.subplots(1,2, figsize = (10,10), dpi = 300)\n",
    "\n",
    "xrange = [np.percentile(res.T[0], 1), np.percentile(res.T[0], 99)]\n",
    "yrange = [np.percentile(res.T[1], 1), np.percentile(res.T[1], 99)]\n",
    "ax[1].set_xlim(xrange[0]-.05,xrange[1]+.05)\n",
    "ax[1].set_ylim(yrange)\n",
    "\n",
    "ax[1].scatter(res.T[0][~y_train[:,0]], res.T[1][~y_train[:,0]], c = \"red\", edgecolor = \"\", s = 1, label = \"full\")\n",
    "ax[1].scatter(res.T[0][y_train[:,0]], res.T[1][y_train[:,0]], c = \"blue\", edgecolor = \"\", s = 1, label = \"empty\")\n",
    "ax[1].legend()\n",
    "\n",
    "ax[0].imshow(manifold)\n",
    "ax[0].set_aspect(1)\n",
    "\n",
    "ax[0].set_xticks(np.linspace(manifold.shape[0]/25/2, manifold.shape[0]-manifold.shape[0]/25/2, 6))\n",
    "ax[0].set_yticks(np.linspace(manifold.shape[0]/25/2, manifold.shape[0]-manifold.shape[0]/25/2, 6))\n",
    "print([np.round(x,1) for x in np.linspace(xrange[0], xrange[1], 6)])\n",
    "ax[0].set_xticklabels([np.round(x,1) for x in np.linspace(xrange[0], xrange[1], 6)])\n",
    "ax[0].set_yticklabels([np.round(x,1) for x in np.linspace(xrange[0], xrange[1], 6)])\n",
    "\n",
    "for a in ax:\n",
    "    a.set_xlabel(\"z [0]\")\n",
    "    a.set_ylabel(\"z [1]\")\n",
    "ax[0].set_aspect(1)\n",
    "ax[1].set_aspect(1)\n",
    "\n",
    "\n",
    "\n",
    "ax[1].set_xlabel(\"z [0]\")\n",
    "ax[1].set_ylabel(\"z [1]\")\n",
    "\n",
    "ax[3].imshow(np.flipud(pos[0]), cmap = \"viridis\")\n",
    "ax[2].imshow(np.flipud(neg[0]), cmap = \"viridis\")\n",
    "ax[4].imshow(np.flipud(pos[0] - neg[0]), cmap = \"seismic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from sklearn.feature_extraction.image import extract_patches_2d\n",
    "\n",
    "arr = np.array(Image.open(\"texture_samples/0.png\"))[200:225, :]\n",
    "windows = extract_patches_2d(arr, (25,25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(windows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2D Fourier transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.ndimage import gaussian_filter\n",
    "sigma = 1\n",
    "phases = np.array([gaussian_filter(np.log(np.abs(np.fft.fft2(w).imag)+1), sigma) for w in windows])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phases -= np.mean(phases, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "def init_fig():\n",
    "    fig, ax = plt.subplots(1, figsize = (8,8))\n",
    "    i = 0\n",
    "    im = ax.imshow(phases[i], cmap =\"viridis\")\n",
    "    ax.axis(\"off\")\n",
    "    return fig, im\n",
    "\n",
    "def get_animate(im):\n",
    "    def animate(i):\n",
    "        im.set_array(phases[i])\n",
    "    return animate\n",
    "init_fig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "import matplotlib.animation as anim\n",
    "from IPython.core.display import display, HTML\n",
    "fig, im = init_fig()\n",
    "ani = anim.FuncAnimation(fig, get_animate(im), frames = 400, interval = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "def plot_animation(vectors, i_min = 0, n_frames = 100):\n",
    "    if vectors[0].ndim == 1:\n",
    "        vectors = np.expand_dims(vectors, 1)\n",
    "    fig = plt.figure()\n",
    "\n",
    "    if np.any(np.array(vectors) < 0):\n",
    "        ylim=(-1, 1)\n",
    "    else:\n",
    "        ylim = (0, np.max(vectors[i_min:i_min+n_frames]))\n",
    "\n",
    "    xmax = len(vectors[0,0])-1\n",
    "    ax = plt.axes(xlim=(0, xmax), ylim = ylim)\n",
    "    lines = []\n",
    "\n",
    "    for i in range(vectors.shape[1]):\n",
    "        line, = ax.plot([], [], lw=2)\n",
    "        lines.append(line)\n",
    "\n",
    "    # initialization function: plot the background of each frame\n",
    "    def init():\n",
    "        for i in range(len(lines)):\n",
    "            lines[i].set_data([], [])\n",
    "        return line,\n",
    "\n",
    "    # animation function.  This is called sequentially\n",
    "    def animate(i):\n",
    "        data = np.array(vectors[i])\n",
    "        for i1, y in enumerate(data):\n",
    "            y = y / np.nanmax([np.nanmax(data), -np.nanmin(data)])\n",
    "            x = np.arange(len(y))\n",
    "            lines[i1].set_data(x, y)\n",
    "        return lines\n",
    "\n",
    "    # call the animator.  blit=True means only re-draw the parts that have changed.\n",
    "    anim = matplotlib.animation.FuncAnimation(fig, animate, init_func=init,\n",
    "                                   frames=n_frames, interval = 100, blit=True)\n",
    "    return anim\n",
    "ani = plot_animation([np.mean(phases[i][:,:12], axis= 1) for i in range(100)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HTML(ani.to_jshtml())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HTML(ani.to_jshtml())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
